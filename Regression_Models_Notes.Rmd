---
title: "Regression Models"
author: "Ryan Hammer"
date: "3/19/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(ggplot2)
library(reshape2)
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Welcome to a set of user created course notes for Regression Models, the 7th of 10 courses in the Johns Hopkins Data Specialization available through Coursera. This document is meant to serve as a study guide in conjunction with watching the instructional videos and attempting the optional Swirl exercises.
For addional resources, visit the community forum on the course page on Coursera, as well as the link below:



Additionally, here are a set of resources that might aid in understanding statistical regression:


Without further ado, we begin.

## Why Regression?

It is important to begin by asking a simple question:

*Why is regression a useful tool?*

Regression can be particularly useful for determining the relationship between two or more variables. Caffo phrases it specifically as "to try to find a parsimonious, easily described mean relationship" when going through example types of questions that can be answered using regression. In this description, parsimonious is being used to describe the results of a regression model as the simple and most straightforward relationship. He contrasts this with a far more complex machine learning model that might be slightly more accurate, but far less useful in the practice of discovering and describing relationships within the data.
Regression also can be used to make inferences about a larger population based on results from a smaller data subset. Such inferences are highly useful in making decisions as well as finding new questions to ask about general populations (in that way one might say regression can create a neverending cycle, but that's not exactly useful, is it?). Lastly, regression is helpful in explaning the tendency of data to trend toward the mean (literally called "regression to the mean").

## Example 1 - Galton's Data

The first major example Caffo explores is height data for an analysis comparing parents and children done by John Galton. The following code loads the required packages for use the of the data set in R.
```{r Galton data}
library(UsingR)
data(Galton)
```
When preparing to do regression on a set of data it is a good practice to do some exploratory work with data. Below are two histograms of height data for children and adults respectively.

```{r pressure, echo=FALSE}
long <- melt(galton)
galt_hist <- ggplot(long, aes(x = value, fill = variable))
galt_hist <- galt_hist + geom_histogram(color = "black", binwidth = 1)
galt_hist <- galt_hist + facet_grid(. ~ variable)
galt_hist
```

This data is going to be used to help introduce and explain the nuts and bolts of the least squares regression model.
Begin by considering the children's heights only. If the goal is to choose the best method for predicting any random child's height, the natural starting place is the middle. It then becomes necessary to describe the middle. The middle is the value for mu that minimizes the sum of squared distances in the formula below:

$\sum_{i=1}^{n} (Y_{i} - \mu)^{2}$

The value of $\mu$ that achieves that is the mean of the data. This value is also equal to the physical center of mass of the histogram above. See the proof below:

$\sum_{i=1}^{n} (Y_{i} - \mu)^{2} = \sum_{i=1}^{n} (Y_{i} - \overline{Y} + \overline{Y} - \mu)^{2}$  
  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
= $\sum_{i=1}^{n} (Y_{i} - \overline{Y})^{2} + 2\sum_{i=1}^{n} (Y_{i} - \overline{Y})(\overline{Y} - \mu) + \sum_{i=1}^{n} (\overline{Y} - \mu)^{2}$  
  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
= $\sum_{i=1}^{n} (Y_{i} - \overline{Y})^{2} + 2(\overline{Y} - \mu)\sum_{i=1}^{n} (Y_{i} - \overline{Y}) + \sum_{i=1}^{n} (\overline{Y} - \mu)^{2}$  
  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
= $\sum_{i=1}^{n} (Y_{i} - \overline{Y})^{2} + 2(\overline{Y} - \mu)(\sum_{i=1}^{n} Y_{i} - n\overline{Y}) + \sum_{i=1}^{n} (\overline{Y} - \mu)^{2}$  
  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
= $\sum_{i=1}^{n} (Y_{i} - \overline{Y})^{2} + \sum_{i=1}^{n} (\overline{Y} - \mu)^{2}$  
  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$\underline{>} \sum_{i=1}^{n} (Y_{i} - \overline{Y})^{2}$  
  
